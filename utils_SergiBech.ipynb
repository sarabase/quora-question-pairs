{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7494ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d653cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data =  os.path.expanduser('~') \n",
    "\n",
    "# use this to train and VALIDATE your solution\n",
    "train_df = pd.read_csv(\"./quora_train_data.csv\")\n",
    "\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(\"./quora_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc04cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41719b",
   "metadata": {},
   "source": [
    "# Out of vocabulary count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81cbb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_count(text, vocab):\n",
    "    \"\"\"\n",
    "    Computes the number of out of vocabulary words in a text given a vocabulary.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The text to compute the OOV counts for.\n",
    "        vocab (set): A set containing the vocabulary of known words.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of out of vocabulary words in the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    oov_words = [word for word in words if word.lower() not in vocab]\n",
    "    return len(oov_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e7ad7",
   "metadata": {},
   "source": [
    "### Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76606c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text to process\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "# Create a set of known words (vocabulary)\n",
    "vocab = set(['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog'])\n",
    "\n",
    "# Compute the OOV count\n",
    "oov_count = oov_count(text, vocab)\n",
    "\n",
    "# Print the results\n",
    "print(f'The text contains {oov_count} out of vocabulary words.')\n",
    "\n",
    "# Add the OOV words to the vocabulary set\n",
    "words = text.split()\n",
    "oov_words = [word.lower() for word in words if word.lower() not in vocab]\n",
    "vocab.update(oov_words)\n",
    "\n",
    "# Print the updated vocabulary set\n",
    "print('Updated vocabulary set:', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test set\n",
    "test_set = 'the quick brown cat jumps over the lazy dog'\n",
    "\n",
    "# Replace OOV words with <unk>\n",
    "words = test_set.split()\n",
    "oov_words = set([word.lower() for word in words if word.lower() not in vocab])\n",
    "test_set_unk = ' '.join(['<unk>' if word.lower() in oov_words else word for word in words])\n",
    "\n",
    "print(test_set_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5284bb",
   "metadata": {},
   "source": [
    "# Rare word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_word_count(text, word_counts, threshold):\n",
    "    \"\"\"\n",
    "    Computes the count of rare words in a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to compute the rare word count for.\n",
    "        word_counts (dict): A dictionary containing the counts of each word in the corpus.\n",
    "        threshold (int): The threshold for a word to be considered \"rare\".\n",
    "        \n",
    "    Returns:\n",
    "        int: The count of rare words in the input text.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Compute the count of rare words\n",
    "    rare_word_count = sum([1 for word in words if word_counts.get(word, 0) < threshold])\n",
    "    \n",
    "    return rare_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b23757",
   "metadata": {},
   "source": [
    "### Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17849594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Load the corpus into memory\n",
    "corpus = ['this is a sentence', 'this is another sentence', 'yet another sentence']\n",
    "\n",
    "# Compute the word counts for the corpus\n",
    "word_counts = collections.Counter()\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    word_counts.update(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe66f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a text to compute the rare word count for\n",
    "text = 'this is a sentence with some rare words'\n",
    "\n",
    "# Compute the count of rare words\n",
    "rare_count = rare_word_count(text, word_counts, 2)\n",
    "\n",
    "# Print the result\n",
    "print(f'The text contains {rare_count} rare words.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af1f9e",
   "metadata": {},
   "source": [
    "# Named entity overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20210f1",
   "metadata": {},
   "source": [
    "Named entity overlap refers to the measure of similarity between two texts based on the number and type of named entities they share. Named entities are words or phrases that refer to specific entities or concepts, such as people, organizations, locations, dates, etc. \n",
    "\n",
    "For example, consider the following two sentences:\n",
    "\n",
    "- John Smith works at Google.\n",
    "- Google is a technology company based in California.\n",
    "\n",
    "Both sentences contain a named entity \"Google\", which is a type of organization. If we calculate the named entity overlap between these two sentences, we would find that they share one named entity in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "def named_entity_overlap(text1, text2):\n",
    "    \"\"\"\n",
    "    Computes the named entity overlap between two texts.\n",
    "    \n",
    "    Args:\n",
    "        text1 (str): The first text.\n",
    "        text2 (str): The second text.\n",
    "        \n",
    "    Returns:\n",
    "        float: The named entity overlap score between the two texts.\n",
    "    \"\"\"\n",
    "    # Tokenize the texts into sentences\n",
    "    sentences1 = nltk.sent_tokenize(text1)\n",
    "    sentences2 = nltk.sent_tokenize(text2)\n",
    "    \n",
    "    # Identify the named entities in each text\n",
    "    entities1 = set()\n",
    "    for sentence in sentences1:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        named_entities = nltk.ne_chunk(tagged, binary=False)\n",
    "        for entity in named_entities:\n",
    "            if isinstance(entity, nltk.tree.Tree):\n",
    "                entity_name = \" \".join([token[0] for token in entity])\n",
    "                entities1.add(entity_name)\n",
    "                \n",
    "    entities2 = set()\n",
    "    for sentence in sentences2:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        named_entities = nltk.ne_chunk(tagged, binary=False)\n",
    "        for entity in named_entities:\n",
    "            if isinstance(entity, nltk.tree.Tree):\n",
    "                entity_name = \" \".join([token[0] for token in entity])\n",
    "                entities2.add(entity_name)\n",
    "    \n",
    "    print('Entities found for text 1: ', entities1)\n",
    "    print('Entities found for text 2: ', entities2)\n",
    "    \n",
    "                \n",
    "    # Compute the named entity overlap between the two texts\n",
    "    overlap = len(entities1.intersection(entities2)) / float(len(entities1.union(entities2)))\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"John Smith works at Google in California.\"\n",
    "text2 = \"Google is a technology company based in California.\"\n",
    "\n",
    "overlap = named_entity_overlap(text1, text2)\n",
    "\n",
    "print(\"Named entity overlap:\", overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49ca09",
   "metadata": {},
   "source": [
    "The score is 0.5 because the two text have 2 entities in common: Google and California, and we have a total of 4 entities. So the named entity overlap is computed as: number of common entites / total number of entities. In this case is 2/4 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"I saw a movie at the AMC theater with my friends.\"\n",
    "text2 = \"I ate dinner at a new Italian restaurant with my family.\"\n",
    "\n",
    "overlap = named_entity_overlap(text1, text2)\n",
    "\n",
    "print(\"Named entity overlap:\", overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9cfb75",
   "metadata": {},
   "source": [
    "# Word2vec and Fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a32e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def compute_word2vec_embeddings(text):\n",
    "    \"\"\"\n",
    "    Computes the word2vec embedding for a given text by taking the mean of embeddings of all the words in the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which the word embeddings need to be computed.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or None: The computed embedding for the given text. If no embeddings are found, returns None.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase and split it into individual words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Initialize empty list for embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for word in words:\n",
    "        # Check if the word is present in the word2vec model's vocabulary\n",
    "        if word in model.index_to_key:\n",
    "            # If the word is present, append its embedding to the list of embeddings\n",
    "            embeddings.append(model[word])\n",
    "\n",
    "    # If no embeddings were found, return None\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # Take the mean of all embeddings to get a single embedding for the entire text\n",
    "        return np.mean(embeddings, axis=0)\n",
    "\n",
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two given word embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding1 (numpy.ndarray or None): The first word embedding.\n",
    "        embedding2 (numpy.ndarray or None): The second word embedding.\n",
    "\n",
    "    Returns:\n",
    "        float or None: The cosine similarity between the two embeddings. If either of the embeddings is None, returns None.\n",
    "    \"\"\"\n",
    "    # Check if either of the embeddings is None\n",
    "    if embedding1 is None or embedding2 is None:\n",
    "        return None\n",
    "    else:\n",
    "        # Compute the cosine similarity between the two embeddings\n",
    "        return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "\n",
    "question1 = \"How can I prevent sunburn?\"\n",
    "question2 = \"How can I prevent sunburn?\"\n",
    "\n",
    "embedding1 = compute_word2vec_embeddings(question1)\n",
    "embedding2 = compute_word2vec_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (same question): {similarity}\")\n",
    "print(\"\")\n",
    "\n",
    "question1 = \"What are some good ways to prevent and treat sunburn?\"\n",
    "question2 = \"How can I prevent sunburn?\"\n",
    "\n",
    "embedding1 = compute_word2vec_embeddings(question1)\n",
    "embedding2 = compute_word2vec_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (similar questions): {similarity}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "question1 = \"How do I learn Python?\"\n",
    "question2 = \"Where is the limit of the universe?\"\n",
    "\n",
    "embedding1 = compute_word2vec_embeddings(question1)\n",
    "embedding2 = compute_word2vec_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (different questions): {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "def compute_fasttext_embeddings(text):\n",
    "    \"\"\"\n",
    "    Computes the FastText embedding for a given text by taking the mean of embeddings of all the words in the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which the word embeddings need to be computed.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or None: The computed embedding for the given text. If no embeddings are found, returns None.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase and split it into individual words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Initialize empty list for embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for word in words:\n",
    "        embeddings.append(ft_model.get_word_vector(word))\n",
    "    # If no embeddings were found, return None\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # Take the mean of all embeddings to get a single embedding for the entire text\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"How can I prevent sunburn?\"\n",
    "question2 = \"How can I prevent sunburn?\"\n",
    "\n",
    "embedding1 = compute_fasttext_embeddings(question1)\n",
    "embedding2 = compute_fasttext_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (same question): {similarity}\")\n",
    "print(\"\")\n",
    "\n",
    "question1 = \"What are some good ways to prevent and treat sunburn?\"\n",
    "question2 = \"How can I prevent sunburn?\"\n",
    "\n",
    "embedding1 = compute_fasttext_embeddings(question1)\n",
    "embedding2 = compute_fasttext_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (similar questions): {similarity}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "question1 = \"How do I learn Python?\"\n",
    "question2 = \"Where is the limit of the universe?\"\n",
    "\n",
    "embedding1 = compute_fasttext_embeddings(question1)\n",
    "embedding2 = compute_fasttext_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (different questions): {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17a95a",
   "metadata": {},
   "source": [
    "### Use other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "distance = euclidean(embedding1, embedding2)\n",
    "similarity = 1 / (1 + distance)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de737a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "distance = cityblock(embedding1, embedding2)\n",
    "similarity = 1 / (1 + distance)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b898e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
