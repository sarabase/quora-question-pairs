{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7494ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d653cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data =  os.path.expanduser('~') \n",
    "\n",
    "# use this to train and VALIDATE your solution\n",
    "train_df = pd.read_csv(\"./quora_train_data.csv\")\n",
    "\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(\"./quora_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc04cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346692</td>\n",
       "      <td>38482</td>\n",
       "      <td>10706</td>\n",
       "      <td>Why do I get easily bored with everything?</td>\n",
       "      <td>Why do I get bored with things so quickly and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>327668</td>\n",
       "      <td>454117</td>\n",
       "      <td>345117</td>\n",
       "      <td>How do I study for Honeywell company recruitment?</td>\n",
       "      <td>How do I study for Honeywell company recruitme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272993</td>\n",
       "      <td>391373</td>\n",
       "      <td>391374</td>\n",
       "      <td>Which search engine algorithm is Quora using?</td>\n",
       "      <td>Why is Quora not using reliable search engine?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54070</td>\n",
       "      <td>82673</td>\n",
       "      <td>95496</td>\n",
       "      <td>How can I smartly cut myself?</td>\n",
       "      <td>Can someone who thinks about suicide for 7 yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46450</td>\n",
       "      <td>38384</td>\n",
       "      <td>72436</td>\n",
       "      <td>How do I see who is viewing my Instagram videos?</td>\n",
       "      <td>Can one tell who viewed my Instagram videos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323427</th>\n",
       "      <td>192476</td>\n",
       "      <td>292119</td>\n",
       "      <td>292120</td>\n",
       "      <td>Is it okay to use a laptop while it is chargin...</td>\n",
       "      <td>Is it OK to use your phone while charging?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323428</th>\n",
       "      <td>17730</td>\n",
       "      <td>33641</td>\n",
       "      <td>33642</td>\n",
       "      <td>How can dogs understand human language?</td>\n",
       "      <td>Can dogs understand the human language?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323429</th>\n",
       "      <td>28030</td>\n",
       "      <td>52012</td>\n",
       "      <td>52013</td>\n",
       "      <td>What's your favourite lotion?</td>\n",
       "      <td>What's your favourite skin lotion?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323430</th>\n",
       "      <td>277869</td>\n",
       "      <td>397054</td>\n",
       "      <td>120852</td>\n",
       "      <td>How does one become a hedge fund manager?</td>\n",
       "      <td>What should I do to become a hedge fund manager?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323431</th>\n",
       "      <td>249342</td>\n",
       "      <td>362958</td>\n",
       "      <td>362959</td>\n",
       "      <td>How did the US acquire over 80 trillion financ...</td>\n",
       "      <td>We've thought about evil geniuses ruling the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323432 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0       346692   38482   10706   \n",
       "1       327668  454117  345117   \n",
       "2       272993  391373  391374   \n",
       "3        54070   82673   95496   \n",
       "4        46450   38384   72436   \n",
       "...        ...     ...     ...   \n",
       "323427  192476  292119  292120   \n",
       "323428   17730   33641   33642   \n",
       "323429   28030   52012   52013   \n",
       "323430  277869  397054  120852   \n",
       "323431  249342  362958  362959   \n",
       "\n",
       "                                                question1  \\\n",
       "0              Why do I get easily bored with everything?   \n",
       "1       How do I study for Honeywell company recruitment?   \n",
       "2           Which search engine algorithm is Quora using?   \n",
       "3                           How can I smartly cut myself?   \n",
       "4        How do I see who is viewing my Instagram videos?   \n",
       "...                                                   ...   \n",
       "323427  Is it okay to use a laptop while it is chargin...   \n",
       "323428            How can dogs understand human language?   \n",
       "323429                      What's your favourite lotion?   \n",
       "323430          How does one become a hedge fund manager?   \n",
       "323431  How did the US acquire over 80 trillion financ...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0       Why do I get bored with things so quickly and ...             1  \n",
       "1       How do I study for Honeywell company recruitme...             1  \n",
       "2          Why is Quora not using reliable search engine?             0  \n",
       "3       Can someone who thinks about suicide for 7 yea...             0  \n",
       "4            Can one tell who viewed my Instagram videos?             1  \n",
       "...                                                   ...           ...  \n",
       "323427         Is it OK to use your phone while charging?             0  \n",
       "323428            Can dogs understand the human language?             0  \n",
       "323429                 What's your favourite skin lotion?             1  \n",
       "323430   What should I do to become a hedge fund manager?             1  \n",
       "323431  We've thought about evil geniuses ruling the w...             0  \n",
       "\n",
       "[323432 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41719b",
   "metadata": {},
   "source": [
    "# Out of vocabulary count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81cbb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_count(text, vocab):\n",
    "    \"\"\"\n",
    "    Computes the number of out of vocabulary words in a text given a vocabulary.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The text to compute the OOV counts for.\n",
    "        vocab (set): A set containing the vocabulary of known words.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of out of vocabulary words in the text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    oov_words = [word for word in words if word.lower() not in vocab]\n",
    "    return len(oov_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e7ad7",
   "metadata": {},
   "source": [
    "### Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76606c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text contains 2 out of vocabulary words.\n",
      "Updated vocabulary set: {'dog', 'jumps', 'lazy', 'quick', 'jumped', 'over', 'brown', 'the', 'fox', 'dog.'}\n"
     ]
    }
   ],
   "source": [
    "# Define a text to process\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "\n",
    "# Create a set of known words (vocabulary)\n",
    "vocab = set(['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog'])\n",
    "\n",
    "# Compute the OOV count\n",
    "oov_count = oov_count(text, vocab)\n",
    "\n",
    "# Print the results\n",
    "print(f'The text contains {oov_count} out of vocabulary words.')\n",
    "\n",
    "# Add the OOV words to the vocabulary set\n",
    "words = text.split()\n",
    "oov_words = [word.lower() for word in words if word.lower() not in vocab]\n",
    "vocab.update(oov_words)\n",
    "\n",
    "# Print the updated vocabulary set\n",
    "print('Updated vocabulary set:', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881f8abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown <unk> jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "# Define a test set\n",
    "test_set = 'the quick brown cat jumps over the lazy dog'\n",
    "\n",
    "# Replace OOV words with <unk>\n",
    "words = test_set.split()\n",
    "oov_words = set([word.lower() for word in words if word.lower() not in vocab])\n",
    "test_set_unk = ' '.join(['<unk>' if word.lower() in oov_words else word for word in words])\n",
    "\n",
    "print(test_set_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5284bb",
   "metadata": {},
   "source": [
    "# Rare word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54d4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_word_count(text, word_counts, threshold):\n",
    "    \"\"\"\n",
    "    Computes the count of rare words in a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to compute the rare word count for.\n",
    "        word_counts (dict): A dictionary containing the counts of each word in the corpus.\n",
    "        threshold (int): The threshold for a word to be considered \"rare\".\n",
    "        \n",
    "    Returns:\n",
    "        int: The count of rare words in the input text.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Compute the count of rare words\n",
    "    rare_word_count = sum([1 for word in words if word_counts.get(word, 0) < threshold])\n",
    "    \n",
    "    return rare_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b23757",
   "metadata": {},
   "source": [
    "### Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17849594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Load the corpus into memory\n",
    "corpus = ['this is a sentence', 'this is another sentence', 'yet another sentence']\n",
    "\n",
    "# Compute the word counts for the corpus\n",
    "word_counts = collections.Counter()\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    word_counts.update(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fe66f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text contains 5 rare words.\n"
     ]
    }
   ],
   "source": [
    "# Define a text to compute the rare word count for\n",
    "text = 'this is a sentence with some rare words'\n",
    "\n",
    "# Compute the count of rare words\n",
    "rare_count = rare_word_count(text, word_counts, 2)\n",
    "\n",
    "# Print the result\n",
    "print(f'The text contains {rare_count} rare words.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af1f9e",
   "metadata": {},
   "source": [
    "# Named entity overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20210f1",
   "metadata": {},
   "source": [
    "Named entity overlap refers to the measure of similarity between two texts based on the number and type of named entities they share. Named entities are words or phrases that refer to specific entities or concepts, such as people, organizations, locations, dates, etc. \n",
    "\n",
    "For example, consider the following two sentences:\n",
    "\n",
    "- John Smith works at Google.\n",
    "- Google is a technology company based in California.\n",
    "\n",
    "Both sentences contain a named entity \"Google\", which is a type of organization. If we calculate the named entity overlap between these two sentences, we would find that they share one named entity in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3852440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: Hostname mismatch, certificate is not valid\n",
      "[nltk_data]     for 'raw.githubusercontent.com'. (_ssl.c:1129)>\n",
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Hostname mismatch, certificate is not valid for\n",
      "[nltk_data]     'raw.githubusercontent.com'. (_ssl.c:1129)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "def named_entity_overlap(text1, text2):\n",
    "    \"\"\"\n",
    "    Computes the named entity overlap between two texts.\n",
    "    \n",
    "    Args:\n",
    "        text1 (str): The first text.\n",
    "        text2 (str): The second text.\n",
    "        \n",
    "    Returns:\n",
    "        float: The named entity overlap score between the two texts.\n",
    "    \"\"\"\n",
    "    # Tokenize the texts into sentences\n",
    "    sentences1 = nltk.sent_tokenize(text1)\n",
    "    sentences2 = nltk.sent_tokenize(text2)\n",
    "    \n",
    "    # Identify the named entities in each text\n",
    "    entities1 = set()\n",
    "    for sentence in sentences1:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        named_entities = nltk.ne_chunk(tagged, binary=False)\n",
    "        for entity in named_entities:\n",
    "            if isinstance(entity, nltk.tree.Tree):\n",
    "                entity_name = \" \".join([token[0] for token in entity])\n",
    "                entities1.add(entity_name)\n",
    "                \n",
    "    entities2 = set()\n",
    "    for sentence in sentences2:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        named_entities = nltk.ne_chunk(tagged, binary=False)\n",
    "        for entity in named_entities:\n",
    "            if isinstance(entity, nltk.tree.Tree):\n",
    "                entity_name = \" \".join([token[0] for token in entity])\n",
    "                entities2.add(entity_name)\n",
    "    \n",
    "    print('Entities found for text 1: ', entities1)\n",
    "    print('Entities found for text 2: ', entities2)\n",
    "    \n",
    "                \n",
    "    # Compute the named entity overlap between the two texts\n",
    "    overlap = len(entities1.intersection(entities2)) / float(len(entities1.union(entities2)))\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2a785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities found for text 1:  {'California', 'John', 'Google', 'Smith'}\n",
      "Entities found for text 2:  {'Google', 'California'}\n",
      "Named entity overlap: 0.5\n"
     ]
    }
   ],
   "source": [
    "text1 = \"John Smith works at Google in California.\"\n",
    "text2 = \"Google is a technology company based in California.\"\n",
    "\n",
    "overlap = named_entity_overlap(text1, text2)\n",
    "\n",
    "print(\"Named entity overlap:\", overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49ca09",
   "metadata": {},
   "source": [
    "The score is 0.5 because the two text have 2 entities in common: Google and California, and we have a total of 4 entities. So the named entity overlap is computed as: number of common entites / total number of entities. In this case is 2/4 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb5b72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities found for text 1:  {'AMC'}\n",
      "Entities found for text 2:  {'Italian'}\n",
      "Named entity overlap: 0.0\n"
     ]
    }
   ],
   "source": [
    "text1 = \"I saw a movie at the AMC theater with my friends.\"\n",
    "text2 = \"I ate dinner at a new Italian restaurant with my family.\"\n",
    "\n",
    "overlap = named_entity_overlap(text1, text2)\n",
    "\n",
    "print(\"Named entity overlap:\", overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9cfb75",
   "metadata": {},
   "source": [
    "# Word2vec and Fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a32e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity (same question): 1\n",
      "\n",
      "Cosine similarity (similar questions): 0.6707423329353333\n",
      "\n",
      "Cosine similarity (different questions): 0.34357038140296936\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def compute_word2vec_embeddings(text):\n",
    "    \"\"\"\n",
    "    Computes the word2vec embedding for a given text by taking the mean of embeddings of all the words in the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which the word embeddings need to be computed.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or None: The computed embedding for the given text. If no embeddings are found, returns None.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase and split it into individual words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Initialize empty list for embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for word in words:\n",
    "        # Check if the word is present in the word2vec model's vocabulary\n",
    "        if word in model.index_to_key:\n",
    "            # If the word is present, append its embedding to the list of embeddings\n",
    "            embeddings.append(model[word])\n",
    "\n",
    "    # If no embeddings were found, return None\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # Take the mean of all embeddings to get a single embedding for the entire text\n",
    "        return np.mean(embeddings, axis=0)\n",
    "\n",
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two given word embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding1 (numpy.ndarray or None): The first word embedding.\n",
    "        embedding2 (numpy.ndarray or None): The second word embedding.\n",
    "\n",
    "    Returns:\n",
    "        float or None: The cosine similarity between the two embeddings. If either of the embeddings is None, returns None.\n",
    "    \"\"\"\n",
    "    # Check if either of the embeddings is None\n",
    "    if embedding1 is None or embedding2 is None:\n",
    "        return None\n",
    "    else:\n",
    "        # Compute the cosine similarity between the two embeddings\n",
    "        return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "\n",
    "question1 = \"How can I prevent sunburn?\"\n",
    "question2 = \"How can I prevent sunburn?\"\n",
    "\n",
    "embedding1 = compute_word2vec_embeddings(question1)\n",
    "embedding2 = compute_word2vec_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (same question): {similarity}\")\n",
    "print(\"\")\n",
    "\n",
    "question1 = \"What are some good ways to prevent and treat sunburn?\"\n",
    "question2 = \"How can I prevent sunburn?\"\n",
    "\n",
    "embedding1 = compute_word2vec_embeddings(question1)\n",
    "embedding2 = compute_word2vec_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (similar questions): {similarity}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "question1 = \"How do I learn Python?\"\n",
    "question2 = \"Where is the limit of the universe?\"\n",
    "\n",
    "embedding1 = compute_word2vec_embeddings(question1)\n",
    "embedding2 = compute_word2vec_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (different questions): {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40bb5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d2d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft_model = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "def compute_fasttext_embeddings(text):\n",
    "    \"\"\"\n",
    "    Computes the FastText embedding for a given text by taking the mean of embeddings of all the words in the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for which the word embeddings need to be computed.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or None: The computed embedding for the given text. If no embeddings are found, returns None.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase and split it into individual words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Initialize empty list for embeddings\n",
    "    embeddings = []\n",
    "\n",
    "    # Iterate through each word in the text\n",
    "    for word in words:\n",
    "        embeddings.append(ft_model.get_word_vector(word))\n",
    "    # If no embeddings were found, return None\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # Take the mean of all embeddings to get a single embedding for the entire text\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c09dc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity (same question): 1\n",
      "\n",
      "Cosine similarity (similar questions): 0.6515657305717468\n",
      "\n",
      "Cosine similarity (different questions): 0.4463950991630554\n"
     ]
    }
   ],
   "source": [
    "question1 = \"How can I prevent sunburn?\"\n",
    "question2 = \"How can I prevent sunburn?\"\n",
    "\n",
    "embedding1 = compute_fasttext_embeddings(question1)\n",
    "embedding2 = compute_fasttext_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (same question): {similarity}\")\n",
    "print(\"\")\n",
    "\n",
    "question1 = \"What are some good ways to prevent and treat sunburn?\"\n",
    "question2 = \"How can I prevent sunburn?\"\n",
    "\n",
    "embedding1 = compute_fasttext_embeddings(question1)\n",
    "embedding2 = compute_fasttext_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (similar questions): {similarity}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "question1 = \"How do I learn Python?\"\n",
    "question2 = \"Where is the limit of the universe?\"\n",
    "\n",
    "embedding1 = compute_fasttext_embeddings(question1)\n",
    "embedding2 = compute_fasttext_embeddings(question2)\n",
    "\n",
    "similarity = compute_cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Cosine similarity (different questions): {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17a95a",
   "metadata": {},
   "source": [
    "### Use other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86f5bd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4115611855086013\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "distance = euclidean(embedding1, embedding2)\n",
    "similarity = 1 / (1 + distance)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5de737a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04907270311233638\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "distance = cityblock(embedding1, embedding2)\n",
    "similarity = 1 / (1 + distance)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b898e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
