{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa20efb",
   "metadata": {},
   "source": [
    "# Utils - David Rosado\n",
    "\n",
    "This notebook contains some functions utils functions and their explanation used for the first project of NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9abffde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.592608Z",
     "start_time": "2023-04-17T12:48:23.684792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Neecessary imports\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "import unicodedata\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065cc958",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f1efc",
   "metadata": {},
   "source": [
    "Text cleaning is the process of preparing text data for analysis by removing or modifying any unwanted or irrelevant information, which can include tasks such as removing punctuations symbols, tokenizaiton, remove stop words, normalize spaces, and the treatment of special tokens. Let me perform some functions in order to deal with this problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377e7010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.600025Z",
     "start_time": "2023-04-17T12:48:24.593976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize a text\n",
    "def tokenize_text(text):\n",
    "    '''\n",
    "    Args: \n",
    "      text (str): The input text to be tokenize\n",
    "    \n",
    "    Returns:\n",
    "      list: The tokenized text in a list\n",
    "    \n",
    "    '''\n",
    "    return [token.lower() for token in nltk.word_tokenize(text)]\n",
    "\n",
    "# Remove punctuation symbols\n",
    "def remove_punctuation(text, question_mark = True):\n",
    "    '''\n",
    "    Args:\n",
    "      text (str): The input text to remove punctuations\n",
    "      question_mark (bool, default=True): If True, the question_mark is removed\n",
    "    \n",
    "    Returns:\n",
    "      str: The final text without punctuation symbols\n",
    "    '''\n",
    "    if question_mark:\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "    else:\n",
    "        return re.sub(r'[^\\w\\s?]', '', text)\n",
    "    \n",
    "# Remove english stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      text (str): The input text to remove stop words\n",
    "    \n",
    "    Returns:\n",
    "      str: The final text without stop words\n",
    "    \"\"\"\n",
    "    # Get the stop words vocabulary\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # Tokenize the text\n",
    "    words = tokenize_text(text)\n",
    "    # Replace\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Replace all consecutive whitespace characters in the text string with a single space.\n",
    "def normalize_spaces(text):\n",
    "    '''\n",
    "    Args:\n",
    "      text (str): The input text to normalize\n",
    "    Returns:\n",
    "      str: The final normalized text\n",
    "    '''\n",
    "    return re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Replace all non-alphabetic characters in the text string with a single space.\n",
    "def remove_nonAlphaWord(text, question_mark = True):\n",
    "    '''\n",
    "    Args:\n",
    "      text (str): The input text to replace non alphabetic characters\n",
    "      question_mark (bool, default=True): If True, the question_mark is removed\n",
    "    Returns:\n",
    "      str : The final replaced text\n",
    "    '''\n",
    "    if question_mark:\n",
    "        return re.sub(r'[^a-zA-Z]', ' ',text)\n",
    "    else: \n",
    "        return re.sub(r'[^a-zA-Z?]', ' ', text)\n",
    "\n",
    "\n",
    "def remove_accents(text):\n",
    "    '''\n",
    "    Args:\n",
    "      text (str): The input text to remove accent\n",
    "    Return:\n",
    "      str : The final text without accents\n",
    "    '''\n",
    "    return unidecode.unidecode(text)\n",
    "\n",
    "# If a token appears less than max_count, we change the word to a general one\n",
    "def special_tokens(corpus, max_count = 1):\n",
    "    '''\n",
    "    Args:\n",
    "      corpus (list): The input corpus to treat special tokens\n",
    "      max_count (int, default = 1): Number of times required for the word to appear. \n",
    "      Otherwise, it is change it to special_token\n",
    "      \n",
    "    Returns:\n",
    "      list: The final corpus as amended\n",
    "    '''\n",
    "    # Count the frequency of each word in the corpus\n",
    "    word_counts = Counter(word for sentence in corpus for word in tokenize_text(sentence))\n",
    "    modified_corpus = []\n",
    "    # Replace single-word occurrences with \"special_token\"\n",
    "    for question in corpus:\n",
    "        modified_sentence = ' '.join('special_token' if word_counts[word] == max_count else word for word in tokenize_text(question))\n",
    "        modified_corpus.append(modified_sentence)\n",
    "\n",
    "    return modified_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54425df8",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let us make some example of the provided functions to understant how it works. Let us start by tokenize some text. Notice that our function returns the tokenize text in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d6c98c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.608793Z",
     "start_time": "2023-04-17T12:48:24.601771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From => This text is an example for the first project of NLP -> ['this', 'text', 'is', 'an', 'example', 'for', 'the', 'first', 'project', 'of', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "\n",
    "txt = 'This text is an example for the first project of NLP'\n",
    "print(f\"From => {txt} -> {tokenize_text(txt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967ec97",
   "metadata": {},
   "source": [
    "Let us continue by showing how it works the remove_punctuation and the remove_nonAlphaWord function. Given some text, the first function will replace any character that is not a word character or whitespace character with nothing. This means that any non-word character (such as punctuation) will be removed entirely, while whitespace characters will be preserved. The second function, remove_nonAlphaWord, will replace any character that is not an English letter with a space. This means that any non-letter character (such as digits, punctuation, or whitespace) will be replaced with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0d098d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.613040Z",
     "start_time": "2023-04-17T12:48:24.609942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From => Wow! This is amazing, actually, it is truly amazing -> Wow This is amazing actually it is truly amazing\n",
      "From => Can I get the ticket please? -> Can I get the ticket please?\n",
      "From => Can I get the ticket please? -> Can I get the ticket please\n",
      "From => I am 100% sure that this is amazing, it is truly amazing -> I am      sure that this is amazing  it is truly amazing\n",
      "From => Can I get the ticket please? -> Can I get the ticket please?\n",
      "From => Can I get the ticket please? -> Can I get the ticket please \n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "txt = 'Wow! This is amazing, actually, it is truly amazing'\n",
    "print(f\"From => {txt} -> {remove_punctuation(txt)}\")\n",
    "txt = 'Can I get the ticket please?'\n",
    "print(f\"From => {txt} -> {remove_punctuation(txt,False)}\")\n",
    "print(f\"From => {txt} -> {remove_punctuation(txt,True)}\")\n",
    "\n",
    "# Remove nonAlpha words\n",
    "txt = 'I am 100% sure that this is amazing, it is truly amazing'\n",
    "print(f\"From => {txt} -> {remove_nonAlphaWord(txt)}\")\n",
    "txt = 'Can I get the ticket please?'\n",
    "print(f\"From => {txt} -> {remove_nonAlphaWord(txt,False)}\")\n",
    "print(f\"From => {txt} -> {remove_nonAlphaWord(txt,True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25947471",
   "metadata": {},
   "source": [
    "Let us continue showing how remove_stopwords works. This is a simple function to remove english stopwords of a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75239bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.617819Z",
     "start_time": "2023-04-17T12:48:24.614193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From => Why I am getting mad with this situation? -> getting mad situation ?\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "txt = 'Why I am getting mad with this situation?'\n",
    "print(f\"From => {txt} -> {remove_stopwords(txt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107ce6e",
   "metadata": {},
   "source": [
    "Let us go on with two more functions: normalize_spaces and remove_accents. The first one replace all consecutive whitespace characters in the text string with a single space and the second one, takes a string containing Unicode characters and returns a new string with those characters replaced by their closest ASCII equivalents. This can be useful for converting non-ASCII text to a more universally readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "807616f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.623594Z",
     "start_time": "2023-04-17T12:48:24.619037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From => This  text  has   many        spaces -> This text has many spaces\n",
      "From => héllo wörld -> hello world\n"
     ]
    }
   ],
   "source": [
    "# Normalize spaces\n",
    "\n",
    "txt = 'This  text  has   many        spaces'\n",
    "print(f\"From => {txt} -> {normalize_spaces(txt)}\")\n",
    "\n",
    "# Remove accents\n",
    "txt = \"héllo wörld\"\n",
    "print(f\"From => {txt} -> {remove_accents(txt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f05ab1",
   "metadata": {},
   "source": [
    "Finally, let me show you how it works the last function, special_tokens. The function takes the whole corpus and starts looking for strange words that only appear once to replace them with special_token. To test it, let us create a little dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77eb44ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.632010Z",
     "start_time": "2023-04-17T12:48:24.624661Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I like to read books\", \"Reading books is enjoyable for me\",\n",
    "    \"She runs every morning\", \"Every morning she goes for a run\",\n",
    "    \"The cat is sleeping\", \"The sleeping cat is cute\",\n",
    "    \"I am learning to code\", \"Coding is a useful skill to learn\",\n",
    "    \"He enjoys playing video games\", \"Playing video games is his favorite hobby\",\n",
    "    \"The car stopped abruptly\", \"The abrupt stop of the car was surprising\",\n",
    "    \"We went to the beach\", \"The beach was crowded and sunny\",\n",
    "    \"She sings beautifully\", \"Her beautiful singing voice is captivating\",\n",
    "    \"The restaurant serves delicious food\", \"The food at the restaurant is always tasty\",\n",
    "    \"He is studying for an exam\", \"Studying is important for academic success\",\n",
    "    \"The flowers are blooming\", \"The blooming flowers are a sign of spring\",\n",
    "    \"The movie was entertaining\", \"I found the movie to be quite enjoyable\",\n",
    "    \"She is a talented musician\", \"Music is her passion and she is very talented\",\n",
    "    \"The building is very tall\", \"The tall building is an impressive feat of engineering\",\n",
    "    \"He traveled to Europe last summer\", \"Last summer he went on a trip to Europe\",\n",
    "    \"I love spending time with my family\", \"My family is very important to me\",\n",
    "    \"The book was very suspenseful\", \"I found the book to be quite thrilling\",\n",
    "    \"She enjoys painting and drawing\", \"Art is her favorite form of self-expression\",\n",
    "    \"The sun is shining brightly today\", \"The bright sun is making everything look beautiful\",\n",
    "    \"He is an excellent chef\", \"Cooking is his passion and he is very skilled\"\n",
    "]\n",
    "dataset_special_tokens = special_tokens(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2abb2f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.638403Z",
     "start_time": "2023-04-17T12:48:24.633287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i special_token to special_token books',\n",
       " 'special_token books is enjoyable for me',\n",
       " 'she special_token every morning',\n",
       " 'every morning she special_token for a special_token',\n",
       " 'the cat is sleeping',\n",
       " 'the sleeping cat is special_token',\n",
       " 'i special_token special_token to special_token',\n",
       " 'special_token is a special_token special_token to special_token',\n",
       " 'he enjoys playing video games',\n",
       " 'playing video games is his favorite special_token',\n",
       " 'the car special_token special_token',\n",
       " 'the special_token special_token of the car was special_token',\n",
       " 'special_token went to the beach',\n",
       " 'the beach was special_token and special_token',\n",
       " 'she special_token special_token',\n",
       " 'her beautiful special_token special_token is special_token',\n",
       " 'the restaurant special_token special_token food',\n",
       " 'the food special_token the restaurant is special_token special_token',\n",
       " 'he is studying for an special_token',\n",
       " 'studying is important for special_token special_token',\n",
       " 'the flowers are blooming',\n",
       " 'the blooming flowers are a special_token of special_token',\n",
       " 'the movie was special_token',\n",
       " 'i found the movie to be quite enjoyable',\n",
       " 'she is a talented special_token',\n",
       " 'special_token is her passion and she is very talented',\n",
       " 'the building is very tall',\n",
       " 'the tall building is an special_token special_token of special_token',\n",
       " 'he special_token to europe last summer',\n",
       " 'last summer he went special_token a special_token to europe',\n",
       " 'i special_token special_token special_token special_token my family',\n",
       " 'my family is very important to me',\n",
       " 'the book was very special_token',\n",
       " 'i found the book to be quite special_token',\n",
       " 'she enjoys special_token and special_token',\n",
       " 'special_token is her favorite special_token of special_token',\n",
       " 'the sun is special_token special_token special_token',\n",
       " 'the special_token sun is special_token special_token special_token beautiful',\n",
       " 'he is an special_token special_token',\n",
       " 'special_token is his passion and he is very special_token']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dac22f",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Let me perform three different text features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32cda5c",
   "metadata": {},
   "source": [
    "# Length information. \n",
    "\n",
    "Let us compute the following:\n",
    "\n",
    "+ Count of number of words for a given text\n",
    "+ Count of non ASCII words for a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "974e19ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.645116Z",
     "start_time": "2023-04-17T12:48:24.642050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of words for a given text\n",
    "def words_count(text):\n",
    "    '''\n",
    "    Args:\n",
    "      text (str): The input text to count the number of words\n",
    "      \n",
    "    Returns:\n",
    "      int : The number of words in the given text\n",
    "    '''\n",
    "    return len(tokenize_text(text))\n",
    "\n",
    "# Number of non ASCII words for a given text\n",
    "def nonAscii_word_count(text):\n",
    "    '''\n",
    "    Args:\n",
    "      text (str): The input text to count the number of non-ASCII words\n",
    "      \n",
    "    Returns:\n",
    "      int : The number of non-ASCII words in the given text\n",
    "    '''\n",
    "    \n",
    "    # Split sentence into words\n",
    "    words = tokenize_text(text)\n",
    "    \n",
    "    # Initialize counter for non-ASCII words\n",
    "    non_ascii_word_count = 0\n",
    "    \n",
    "    # Loop through words and check if each one contains non-ASCII characters\n",
    "    for word in words:\n",
    "        # Normalize the word to its canonical form (NFKD) to separate diacritics\n",
    "        normalized_word = unicodedata.normalize('NFKD', word)\n",
    "        # Check if any character in the normalized word has a non-ASCII category\n",
    "        if any(not c.isascii() for c in normalized_word):\n",
    "            non_ascii_word_count += 1\n",
    "    \n",
    "    return non_ascii_word_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dcc906",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be7f323",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.648279Z",
     "start_time": "2023-04-17T12:48:24.646192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Word count\n",
    "txt = 'This text contains five words'\n",
    "print(words_count(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b28bda79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.652109Z",
     "start_time": "2023-04-17T12:48:24.649486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Non - ASCII word count\n",
    "txt = 'The café serves croissants and café au lait.'\n",
    "print(nonAscii_word_count(txt))\n",
    "txt = 'This is an example text with some non-ASCII words like café, résumé, Pokémon and 阿.'\n",
    "print(nonAscii_word_count(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c1969",
   "metadata": {},
   "source": [
    "# Common word intersection count\n",
    "\n",
    "Let us make a function that calculates the number of common words that two sentences have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb965b24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.656802Z",
     "start_time": "2023-04-17T12:48:24.654275Z"
    }
   },
   "outputs": [],
   "source": [
    "def common_words_count(text1,text2):\n",
    "    '''\n",
    "    Args:\n",
    "      text1 (str): First sentence\n",
    "      text2 (str): Second sentence\n",
    "    \n",
    "    Return:\n",
    "      int: The number of common words that the two sentences have in common\n",
    "    '''\n",
    "    # Compute the tokens for each sentence\n",
    "    tokens1 = set(tokenize_text(text1))\n",
    "    tokens2 = set(tokenize_text(text2))\n",
    "    \n",
    "    # Return the number of common words\n",
    "    return len(tokens1 & tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32f763",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb45c93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.660805Z",
     "start_time": "2023-04-17T12:48:24.657942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common words count example\n",
    "txt1 = 'This is a sentence to taste the implemented function'\n",
    "txt2 = 'The aim of this sentence is to taste the implemented function'\n",
    "'''\n",
    "Common_words = {'function', 'implemented', 'is', 'sentence', 'taste', 'the', 'this', 'to'}\n",
    "'''\n",
    "common_words_count(txt1,txt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b0c7fe",
   "metadata": {},
   "source": [
    "# Study of the beginning of the question\n",
    "\n",
    "let us look at whether the start of the question is one of the following tokens: Who, Where, When, Why, What, Which, How. Let us create two different approaches in order to deal with this. The first one is to create a one hot encoding of the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ca9956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.665573Z",
     "start_time": "2023-04-17T12:48:24.662005Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_begin(corpus):\n",
    "    '''\n",
    "    Args:\n",
    "      corpus (list): The whole corpus to create the one hot encoding\n",
    "    \n",
    "    Return:\n",
    "      dataframe : A dataframe with the one hot encoding\n",
    "    '''\n",
    "    # Define the one-hot encoding labels\n",
    "    labels = ['who', 'where', 'when', 'why', 'what', 'which', 'how']\n",
    "    \n",
    "    # Initialize an empty list to store the one-hot encodings\n",
    "    one_hot_encodings = []\n",
    "    \n",
    "    # Iterate through each sentence in the dataset\n",
    "    for question in corpus:\n",
    "        # Initialize a list of zeros\n",
    "        one_hot_encoding = [0] * len(labels)\n",
    "        \n",
    "        # Split the sentence into individual words\n",
    "        words = tokenize_text(question)\n",
    "        \n",
    "        # Check if the first word of the sentence is in the labels list\n",
    "        if words[0] in labels:\n",
    "            one_hot_encoding[labels.index(words[0])] = 1\n",
    "        \n",
    "        # Add the one-hot encoding to the list of encodings\n",
    "        one_hot_encodings.append(one_hot_encoding)\n",
    "    \n",
    "    # Convert the list of encodings to a pandas dataframe\n",
    "    df_one_hot = pd.DataFrame(one_hot_encodings, columns=labels)\n",
    "    \n",
    "    return df_one_hot\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79d629",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c78d4f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:48:24.675475Z",
     "start_time": "2023-04-17T12:48:24.666721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>who</th>\n",
       "      <th>where</th>\n",
       "      <th>when</th>\n",
       "      <th>why</th>\n",
       "      <th>what</th>\n",
       "      <th>which</th>\n",
       "      <th>how</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   who  where  when  why  what  which  how\n",
       "0    0      0     0    0     0      0    1\n",
       "1    0      0     0    0     0      0    0\n",
       "2    0      0     1    0     0      0    0\n",
       "3    0      0     0    1     0      0    0\n",
       "4    0      0     0    0     0      0    0\n",
       "5    0      0     0    0     1      0    0\n",
       "6    0      1     0    0     0      0    0\n",
       "7    0      0     0    0     0      0    0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a little dataset for test it\n",
    "corpus = [\n",
    "    'How do you do?', 'Shoud we play chess?',\n",
    "    'When did you arrive?', 'Why are you crazy?',\n",
    "    'Oh, is that you?', 'What about you?',\n",
    "    'Where is the nearest restaurant', 'Amazing']\n",
    "\n",
    "one_hot_begin(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e56f1f9",
   "metadata": {},
   "source": [
    "An, simply returns True/False, if two questions starts with the same word or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8e34ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:51:31.567453Z",
     "start_time": "2023-04-17T12:51:31.561407Z"
    }
   },
   "outputs": [],
   "source": [
    "def frist_word_is_same(text1,text2):\n",
    "    '''\n",
    "    Args:\n",
    "      text1 (str): First sentence\n",
    "      text2 (str): Second sentence\n",
    "    \n",
    "    Returns:\n",
    "      bool: True if the first word is the same, otherwise, False\n",
    "    '''\n",
    "    # Tokenize the text\n",
    "    tokens1 = tokenize_text(text1)\n",
    "    tokens2 = tokenize_text(text2)\n",
    "    # Return True/False\n",
    "    return tokens1[0] == tokens2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525bf8a",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dafa8d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T12:51:32.952722Z",
     "start_time": "2023-04-17T12:51:32.943055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if two questions start with the same word\n",
    "\n",
    "txt1 = 'How are you?'\n",
    "txt2 = 'How are you doing?'\n",
    "print(frist_word_is_same(txt1,txt2))\n",
    "\n",
    "txt1 = 'Why are you here?'\n",
    "txt2 = 'Where is the party?'\n",
    "print(frist_word_is_same(txt1,txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c4646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
